{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternetFirewallSupervisedLearning\n",
    "## Author: Tijs van Lieshout\n",
    "\n",
    "Predicting the Internet Firewall action based on log info. Supervised Learning Assignment for Master DSLS.\n",
    "\n",
    "### Data availability:\n",
    "- [Direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/00542/log2.csv)\n",
    "- [Archive link with some info](https://archive.ics.uci.edu/ml/datasets/Internet+Firewall+Data)\n",
    "- [F. Ertam and M. Kaya, \"Classification of firewall log files with multiclass support vector machine,\" 2018 6th International Symposium on Digital Forensic and Security (ISDFS), 2018, pp. 1-4, doi: 10.1109/ISDFS.2018.8355382.](https://doi.org/10.1109/ISDFS.2018.8355382)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1. Features and Description. Adapted from Ertam & Kaya, 2018.\n",
    "\n",
    "|        Feature       |                  Description                 |\n",
    "|:--------------------:|:--------------------------------------------:|\n",
    "| Source Port          | Client Source Port                           |\n",
    "| Destination Port     | Client Destination Port                      |\n",
    "| NAT Source Port      | Network Address Translation Source Port      |\n",
    "| NAT Destination Port | Network Address Translation Destination Port |\n",
    "| Elapsed Time (sec)   | Elapsed Time for flow                        |\n",
    "| Bytes                | Total Bytes                                  |\n",
    "| Bytes Sent           | Bytes Sent                                   |\n",
    "| Bytes Received       | Bytes Received                               |\n",
    "| Packets              | Total Packets                                |\n",
    "| pkts_sent            | Packets Sent                                 |\n",
    "| pkts_received        | Packets Received                             |\n",
    "| Action               | Class (allow, deny, drop, reset-both)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2. Classes to predict (Actions). Adapted from Ertam & Kaya, 2018.\n",
    "\n",
    "|   Action   |                                                               Description                                                              |\n",
    "|:----------:|:--------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| Allow      | Allows the internet traffic.                                                                                                           |\n",
    "| Deny       | Blocks traffic and enforces the default Deny Action defined for the application that is being denied.                                  |\n",
    "| Drop       | Silently drops the traffic; for an application, it overrides the default deny action. A TCP reset is not sent to the host/application. |\n",
    "| Reset-Both | Sends a TCP reset to both the client-side and server-sidedevices.                                                                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"log2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allow, deny, drop, reset_both = df['Action'].value_counts()\n",
    "print('Number of allowed logs: ', allow)  \n",
    "print('Number of denied logs: ', deny)\n",
    "print('Number of dropped logs: ', drop)  \n",
    "print('Number of reset-both logs: ', reset_both) \n",
    "\n",
    "print('\\n')\n",
    "print('% of allowed logs', round(allow / len(df) * 100, 1), '%')\n",
    "print('% of denied logs', round(deny / len(df) * 100, 1), '%')\n",
    "print('% of dropped logs', round(drop / len(df) * 100, 1), '%')\n",
    "print('% of reset-both logs', round(reset_both / len(df) * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Action'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty unbalanced classes to predict unfortunately. Might run in some problems with the reset-both class later on. SMOTE could be used, but in this case I won't attempt it (out of the scope of this assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data\n",
    "df.isnull().sum() \n",
    "# no missing data, no imputation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "description = df.groupby(['Action']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df.corr().abs()\n",
    "sns.heatmap(c, cmap=sns.color_palette(\"Blues\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Bytes Sent', 'Bytes Received', 'pkts_sent' and 'pkts_received' can be discarded as Bytes and Packets are the total of the two pairs respectively.\n",
    "\n",
    "I am also going to discard packets for bytes as it is highly correlated. I'll keep Bytes since it is more detailed than packets (1 packet consists of multiple bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the port variables should not be seen as continous, but probably are interesting to see the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"Source Port\", hue=\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description['Source Port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Actions of drop seem to happen in high source ports (minimum 49156). Reset-both Source Port minimum is 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"Destination Port\", hue=\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description['Destination Port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most actions seem to have a very low destination port. All drop actions are done on Destination Port 445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"NAT Source Port\", hue=\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description['NAT Source Port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowed actions seme to be uniformly distributed over NAT Source Ports. All dropped NAT Source Ports are equal to 0. Most deny and reset-both actions have NAT Source Ports of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=\"NAT Destination Port\", hue=\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description['NAT Destination Port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowed actions seem to be distributed over all NAT Destination Ports, but mostly under 443. All dropped NAT Destination Ports are equal to 0. Most deny and reset-both actions have NAT Destination Ports of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Source Port', \n",
    "        'Destination Port', \n",
    "        'NAT Source Port', \n",
    "        'NAT Destination Port', \n",
    "        'Bytes', \n",
    "        'Elapsed Time (sec)']\n",
    "df_features = df[cols].rename(columns={'Source Port':'source_port',\n",
    "                                       'Destination Port':'destination_port', \n",
    "                                       'NAT Source Port':'nat_source_port', \n",
    "                                       'NAT Destination Port':'nat_destination_port',\n",
    "                                       'Bytes':'bytes',\n",
    "                                       'Elapsed Time (sec)':'elapsed_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_features.corr().abs()\n",
    "sns.heatmap(c, cmap=sns.color_palette(\"Blues\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array(df['Action'].replace({'allow':0,'deny':1,'drop':2, 'reset-both':3}))\n",
    "X = np.array(df_features)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize because the variance differs greatly per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize(X):\n",
    "    scalar = StandardScaler()\n",
    "    scalar = scalar.fit(X)\n",
    "    X = scalar.transform(X)\n",
    "    return X\n",
    "\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "\n",
    "#split\n",
    "test_size = 0.4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "\n",
    "#cross validation\n",
    "cv = ShuffleSplit(n_splits=100, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate(y_test, y_pred, X_test):\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on code from https://github.com/fenna/student_BFVM19DATASC3\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train, X_val, y_val, training_sizes=range(999, len(X_train), 1000)):\n",
    "    MSE_train, MSE_val = calculate_MSE_over_training_sizes(model, X_train, y_train, \n",
    "                                                           X_val, y_val, training_sizes)\n",
    "\n",
    "    plt.plot(training_sizes, MSE_train,\n",
    "             \"r-o\", linewidth=2, label=\"trainingsdata\")\n",
    "    plt.plot(training_sizes, MSE_val, \n",
    "             \"b-*\", linewidth=3, label=\"validationdata\")\n",
    "    plt.legend(loc=\"best\", fontsize=14)   \n",
    "    plt.xlabel(\"Training set size\", fontsize=14) \n",
    "    plt.ylabel(\"RMSE\", fontsize=14) \n",
    "\n",
    "    \n",
    "def calculate_MSE_over_training_sizes(model, X_train, y_train, X_val, y_val, training_sizes):\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in training_sizes:\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "        \n",
    "    return np.sqrt(train_errors), np.sqrt(val_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most basic model; A dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DummyClassifier(strategy='stratified') \n",
    "# stratified generates predictions by respecting the training setâ€™s class distribution.\n",
    "dm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dm.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(dm, X_train, y_train, X_test, y_test, range(1999, len(X_train), 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve you can not say if the model is underfitted or overfitted as it actually doesn't learn over iterations since it is a dummy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on from this dummy model and attempt a basic classifier; Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression(max_iter=1000)\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "y_pred = lg.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning basically means that there are not enough instances of the fourth class to have in both the training and test set, so it cannot evaluate it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(lg, X_train, y_train, X_test, y_test, range(1999, len(X_train), 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is fitted pretty good as the validation data and training data error are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fourth class could not be evaluated, let's make a decision tree which will at least attempt it albeit the low support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(dt, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is underfitted as there is still a large gap between training error and validation error. Although mait looks like more data would not fix this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get to the real deal, comparing SVM methods to Ertam & Kaya, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function: linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_lin = SVC(kernel='linear')\n",
    "svm_lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_lin.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(svm_lin, X_train, y_train, X_test, y_test, range(4999, len(X_train), 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is roughly fitted good. No large gap in error between training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function: poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_poly = SVC(kernel='poly')\n",
    "svm_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_poly.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning basically means that there are not enough instances of the fourth class to have in both the training and test set, so it cannot evaluate it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(svm_poly, X_train, y_train, X_test, y_test, range(4999, len(X_train), 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is roughly fitted good. No large gap in error between training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function: RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = svm_rbf.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning basically means that there are not enough instances of the fourth class to have in both the training and test set, so it cannot evaluate it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(svm_rbf, X_train, y_train, X_test, y_test, range(4999, len(X_train), 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is roughly fitted good. No large gap in error between training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function: sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_sig = SVC(kernel='sigmoid')\n",
    "svm_sig.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = svm_sig.predict(X_test)\n",
    "evaluate(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning basically means that there are not enough instances of the fourth class to have in both the training and test set, so it cannot evaluate it's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(svm_sig, X_train, y_train, X_test, y_test, range(4999, len(X_train), 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this learning curve I would say this model is underfitted as there is still a large gap between training error and validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3: Comparison of SVM performance between Ertam & Kaya, 2018 and my own implementation.\n",
    "\n",
    "\\* = Better performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n",
    ".tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}\n",
    ".tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-uzvj\" rowspan=\"2\">Method</th>\n",
    "    <th class=\"tg-7btt\" colspan=\"2\">F1</th>\n",
    "    <th class=\"tg-7btt\" colspan=\"2\">Precision</th>\n",
    "    <th class=\"tg-7btt\" colspan=\"2\">Recall</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">Ertam &amp; Kaya</td>\n",
    "    <td class=\"tg-7btt\">van Lieshout</td>\n",
    "    <td class=\"tg-7btt\">Ertam &amp; Kaya</td>\n",
    "    <td class=\"tg-7btt\">van Lieshout</td>\n",
    "    <td class=\"tg-7btt\">Ertam &amp; Kaya</td>\n",
    "    <td class=\"tg-7btt\">van Lieshout</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">SVM Linear</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "    <td class=\"tg-0pky\">0.99*</td>\n",
    "    <td class=\"tg-0pky\">0.68</td>\n",
    "    <td class=\"tg-0pky\">0.99<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.99<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">SVM Polynomial</td>\n",
    "    <td class=\"tg-0pky\">0.53</td>\n",
    "    <td class=\"tg-0pky\">0.98<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.62</td>\n",
    "    <td class=\"tg-0pky\">0.98<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.47</td>\n",
    "    <td class=\"tg-0pky\">0.98<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"><span style=\"font-weight:400;font-style:normal\">SVM RBF</span></td>\n",
    "    <td class=\"tg-0pky\">0.76</td>\n",
    "    <td class=\"tg-0pky\">0.99<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.63</td>\n",
    "    <td class=\"tg-0pky\">0.99<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.97</td>\n",
    "    <td class=\"tg-0pky\">0.99<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"><span style=\"font-weight:400;font-style:normal\">SVM Sigmoid</span></td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "    <td class=\"tg-0pky\">0.84<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.60</td>\n",
    "    <td class=\"tg-0pky\">0.84<span style=\"font-weight:400;font-style:normal\">*</span></td>\n",
    "    <td class=\"tg-0pky\">0.99*</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Except for the recall of the SVM Sigmoid I have improved all other metrics for all other activation types of multi-class Support Vector Machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
